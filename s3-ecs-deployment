## Update Silodown's Build & Deploy Code
The goal is to update how we deploy code continue to push the API to the backend ecs service we have configured current, along with a react based front end application to S3 served through cloudfront. 

I've accomplished this before, but need to adapt it to this project, but the goals are nearly identical.  Below are example scripts from a previous project that I would like you to use as inspiration, as I do want to manage versions show in the examples in the same way. 

The primary area where we are updating code is /app/infra-silodown.  

## Ensure there are example bash script usage instructions in a README or some other useful form after any changes. 


## Example Deploy Bash Script 
This script is what runs the whole deployment, and one of the subprocesses is the CDK deployment portion. 
```
#!/bin/bash
set -e # Exit immediately if a command exits with a non-zero status

# Parse command line arguments
PROFILE="redline"  # Default profile
ENVIRONMENT=${ENVIRONMENT:-"dev"}  # Default to dev if not set
SKIP_BUILD=false  # Default to building images

# Process command line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --profile=*)
      PROFILE="${1#*=}"
      shift
      ;;
    --profile)
      PROFILE="$2"
      shift 2
      ;;
    --environment=*|--env=*)
      ENVIRONMENT="${1#*=}"
      shift
      ;;
    --environment|--env)
      ENVIRONMENT="$2"
      shift 2
      ;;
    --skip-build)
      SKIP_BUILD=true
      shift
      ;;
    *)
      # Save other arguments for passing to CDK later
      ARGS+=($1)
      shift
      ;;
  esac
done

echo "Using AWS profile: $PROFILE"
echo "Using environment: $ENVIRONMENT"
if [ "$SKIP_BUILD" = true ]; then
  echo "Skipping build process, using last available images"
fi

# Check if .env file exists
if [ -f ../.env ]; then
    # Load environment variables from .env file
    export $(cat ../.env | grep -v '^#' | xargs)
    echo "Loaded environment variables from ../.env:"
    cat ../.env | grep -v '^#' | sed 's/=.*/=***/' # Print variable names but hide values
else
    echo "No .env file found"
    exit 1
fi

# Define variables
REGION="us-east-1"  # Hardcode the region
export AWS_DEFAULT_REGION=$REGION  # Set default region for AWS CLI
VERSION=${VERSION:-""}

# Get or create ECR repository with environment
REPO_NAME="wholesale-api-${ENVIRONMENT}"
echo "Checking ECR repository for environment: ${ENVIRONMENT}..."
ECR_REPO=$(aws ecr describe-repositories --profile $PROFILE --region $REGION --repository-names $REPO_NAME --query 'repositories[0].repositoryUri' --output text 2>/dev/null || \
    aws ecr create-repository --profile $PROFILE --region $REGION \
        --repository-name $REPO_NAME \
        --query 'repository.repositoryUri' \
        --output text)

if [ -z "$ECR_REPO" ]; then
    echo "Failed to create or find ECR repository"
    exit 1
fi

# Export ECR_REPO for CDK
export ECR_REPO

echo "Using ECR repository: $ECR_REPO"

# Determine version based on ECR repository tags
if [ -z "$VERSION" ]; then
    # Only increment version if we're building (not skipping build)
    if [ "$SKIP_BUILD" = false ]; then
        # Try to get the latest version tag from ECR
        echo "Looking for version tags in repository: $REPO_NAME"
        
        # Get all tags and filter for semantic version format
        ALL_TAGS=$(aws ecr describe-images --profile $PROFILE --region $REGION --repository-name $REPO_NAME \
            --query 'imageDetails[*].imageTags[*]' --output text 2>/dev/null)
            
        echo "All tags found: $ALL_TAGS"
        
        # Extract version tags and find the latest
        LATEST_VERSION=$(echo "$ALL_TAGS" | tr '\t' '\n' | grep -E '^[0-9]+\.[0-9]+\.[0-9]+$' | sort -V | tail -n 1)
        
        echo "Latest version tag found: ${LATEST_VERSION:-None}"
        
        if [ -z "$LATEST_VERSION" ]; then
            # No valid version tag found, start with 1.0.0
            VERSION="1.0.0"
            echo "No existing version found in ECR, starting with: $VERSION"
        else
            # Split version into parts
            IFS='.' read -r -a VERSION_PARTS <<< "$LATEST_VERSION"
            
            # Increment minor version (second digit)
            VERSION_PARTS[1]=$((VERSION_PARTS[1] + 1))
            # Reset patch version
            VERSION_PARTS[2]=0
            
            # Reassemble version string
            VERSION="${VERSION_PARTS[0]}.${VERSION_PARTS[1]}.${VERSION_PARTS[2]}"
            echo "Incrementing version to: $VERSION"
        fi
    else
        # For skip-build, use latest version from ECR without incrementing
        echo "Looking for version tags in repository: $REPO_NAME"
        
        # Get all tags and filter for semantic version format
        ALL_TAGS=$(aws ecr describe-images --profile $PROFILE --region $REGION --repository-name $REPO_NAME \
            --query 'imageDetails[*].imageTags[*]' --output text 2>/dev/null)
            
        echo "All tags found: $ALL_TAGS"
        
        # Extract version tags and find the latest
        LATEST_VERSION=$(echo "$ALL_TAGS" | tr '\t' '\n' | grep -E '^[0-9]+\.[0-9]+\.[0-9]+$' | sort -V | tail -n 1)
        
        echo "Latest version tag found: ${LATEST_VERSION:-None}"
        
        if [ -z "$LATEST_VERSION" ]; then
            VERSION="1.0.0"
            echo "No existing version found in ECR, using: $VERSION"
        else
            VERSION="$LATEST_VERSION"
            echo "Using existing version: $VERSION (not incrementing because build is skipped)"
        fi
    fi
fi

# Write the version to a version.txt file in the api directory
echo "Writing version $VERSION to ../api/src/version.txt"
echo "$VERSION" > "../api/src/version.txt"
echo "Environment: $ENVIRONMENT" >> "../api/src/version.txt"
echo "Build timestamp: $(date)" >> "../api/src/version.txt"

# Only build and push images if not skipping build
if [ "$SKIP_BUILD" = false ]; then
  # Create a new builder instance
  docker buildx create --use
  docker buildx inspect --bootstrap
  
  # Login to ECR
  aws ecr get-login-password --profile $PROFILE --region $REGION | docker login --username AWS --password-stdin $ECR_REPO
  
  # Build and push the API Docker image
  echo "Building and pushing to ECR: $ECR_REPO:$VERSION (environment: $ENVIRONMENT)"
  cd ../api
  docker buildx build --platform linux/arm64 \
      -t $ECR_REPO:$VERSION \
      -t $ECR_REPO:$ENVIRONMENT \
      -t $ECR_REPO:latest \
      -f Dockerfile \
      . --push
  
  # Build the React frontend for S3 deployment
  echo "Building React frontend for S3 deployment (environment: $ENVIRONMENT)"
  cd ../web-front-end
  
  # Determine the API URL based on environment
  if [ "$ENVIRONMENT" = "prod" ]; then
    API_URL="https://api.autoturn.ai"
  else
    API_URL="https://$ENVIRONMENT-api.autoturn.ai"
  fi
  
  # Create .env file for Vite build with the correct API URL
  echo "VITE_API_URL=$API_URL" > .env
  
  # Install dependencies if needed
  npm install
  
  # Build the frontend with production settings
  echo "Building frontend with VITE_API_URL=$API_URL"
  npm run build
  
  # If build failed, exit with error
  if [ $? -ne 0 ]; then
    echo "Frontend build failed. Exiting..."
    exit 1
  fi
  
  cd ../infra-wholesale
  
  echo "Build and push to ECR complete. Image: $ECR_REPO:$VERSION"
  echo "Frontend build complete and ready for S3 deployment"
else
  echo "Skipping build process, using existing image: $ECR_REPO:$VERSION"
  # Still need to change directory back to infra-wholesale if we're in the parent directory
  if [ "$(basename "$(pwd)")" != "infra-wholesale" ]; then
    cd infra-wholesale
  fi
fi

# Deploy the stack with environment context and specified profile
# Deploy the stack with CDK
echo "Deploying CDK stack with environment: $ENVIRONMENT, ECR repo: $ECR_REPO"
# Get current public IP address
MY_IP=$(curl -s https://api.ipify.org)
echo "Using IP address for DB ingress: $MY_IP"

npx cdk deploy \
  --profile $PROFILE \
  --region $REGION \
  --context environment=$ENVIRONMENT \
  --context ecrRepo=$ECR_REPO \
  --context version=$VERSION \
  --context allowedIp=$MY_IP \
  ${ARGS[@]:-}

# Clean up Docker buildx builders only if we built images
if [ "$SKIP_BUILD" = false ]; then
  echo "Cleaning up Docker buildx builders..."
  docker buildx prune -f
fi
```

### CDK Build / Stack Script
```
import * as cdk from "aws-cdk-lib";
import {
  aws_rds,
  aws_ec2,
  aws_secretsmanager,
  CfnOutput,
  aws_certificatemanager,
  aws_route53,
  aws_route53_targets,
  aws_iam,
  aws_s3,
  aws_s3_deployment,
  aws_cloudfront,
  aws_cloudfront_origins,
  aws_logs,
  aws_kms,
  aws_ecr,
  aws_ecs,
  aws_elasticloadbalancingv2,
} from "aws-cdk-lib";
import { Construct } from "constructs";

export interface InfraWholesaleStackProps extends cdk.StackProps {
  apiKey?: string;
  region?: string;
  stage?: string;
}

export class InfraWholesaleStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: InfraWholesaleStackProps) {
    super(scope, id, props);

    // Retrieve the environment context
    const environment = this.node.tryGetContext("environment") || "dev"; // default to "dev" if not provided
    const allowedIp = this.node.tryGetContext("allowedIp") || "0.0.0.0";
    const version = this.node.tryGetContext("version") || "1.0.0";
    const prefix = `wholesale-${environment}`; // Prefix for resource names

    // Environment variables are available via process.env
    const {
      OPENAI_API_KEY,
      POSTGRES_USER,
      POSTGRES_DB,
      SESSION_SECRET,
      SEND_GRID_FROM_EMAIL,
      SEND_GRID_API_KEY: SENDGRID_API_KEY,
      ENV,
      DOMAIN_NAME,
      EMAIL,
      POSTMARK_API_KEY,
      ECR_REPO,
      APP_NAME
    } = process.env;

    // Get account ID for ECR repository
    const accountId = cdk.Stack.of(this).account;
    const region = cdk.Stack.of(this).region;
    const defaultEcrRepo = `${accountId}.dkr.ecr.${region}.amazonaws.com/wholesale-api`;
    
    // Use provided ECR_REPO or construct default one
    const ecrRepo = ECR_REPO || defaultEcrRepo;

    const vpc = new aws_ec2.Vpc(this, `${prefix}-vpc`, {
      maxAzs: 2,
      natGateways: 1,
    });

    // Create a security group for the ECS service
    const ecsSecurityGroup = new aws_ec2.SecurityGroup(
      this,
      `${prefix}-ecs-sg`,
      {
        vpc,
        allowAllOutbound: true,
        description: "Security group for ECS Fargate service",
      },
    );

    // Allow inbound traffic on port 80 for the ECS service
    ecsSecurityGroup.addIngressRule(
      aws_ec2.Peer.anyIpv4(),
      aws_ec2.Port.tcp(80),
      "Allow inbound HTTP traffic",
    );

    // Allow inbound traffic on port 443 for the ECS service
    ecsSecurityGroup.addIngressRule(
      aws_ec2.Peer.anyIpv4(),
      aws_ec2.Port.tcp(443),
      "Allow inbound HTTPS traffic",
    );

    // Create a security group for RDS
    const dbSecurityGroup = new aws_ec2.SecurityGroup(
      this,
      `${prefix}-db-sg`,
      {
        vpc,
        allowAllOutbound: true,
        description: "Security group for RDS instance",
      },
    );

    // Allow inbound access from ECS security group to RDS
    dbSecurityGroup.addIngressRule(
      ecsSecurityGroup,
      aws_ec2.Port.tcp(5432),
      "Allow PostgreSQL access from ECS",
    );
    
    // Allow inbound access from the entire VPC CIDR range
    dbSecurityGroup.addIngressRule(
      aws_ec2.Peer.ipv4(vpc.vpcCidrBlock),
      aws_ec2.Port.tcp(5432),
      "Allow PostgreSQL access from all resources in the VPC",
    );

    // Allow inbound access from your IP address
    dbSecurityGroup.addIngressRule(
      aws_ec2.Peer.ipv4(allowedIp+"/32"),  
      aws_ec2.Port.tcp(5432),
      "Allow PostgreSQL access from development machine",
    );

    // Step 4: Create a Secrets Manager Secret for DB credentials
    const dbCredentialsSecret = new aws_secretsmanager.Secret(
      this,
      `${prefix}-DbCredentialsSecret`,
      {
        secretName: `${prefix}-RdsDbCredentials`,
        generateSecretString: {
          secretStringTemplate: JSON.stringify({ username: POSTGRES_USER }),
          generateStringKey: "password",
          excludeCharacters: "\"@/\\ '",
        },
      },
    );

    // PostgreSQL 16 has built-in support for pgvector
    // No special parameter group needed - will create the extension manually after deployment

    // Step 7: Create a RDS PostgreSQL instance with vector support
    const dbInstance = new aws_rds.DatabaseInstance(
      this,
      `${prefix}-RdsInstance`,
      {
        engine: aws_rds.DatabaseInstanceEngine.postgres({
          version: aws_rds.PostgresEngineVersion.VER_16,
        }),
        instanceType: aws_ec2.InstanceType.of(
          aws_ec2.InstanceClass.T3,
          aws_ec2.InstanceSize.MEDIUM,
        ),
        vpc,
        vpcSubnets: { subnetType: aws_ec2.SubnetType.PUBLIC },
        securityGroups: [dbSecurityGroup],
        credentials: aws_rds.Credentials.fromSecret(dbCredentialsSecret),
        databaseName: process.env.POSTGRES_DB || 'wholesale',
        removalPolicy: cdk.RemovalPolicy.DESTROY,
        deletionProtection: false,
        multiAz: false,
        allocatedStorage: 20,
        storageType: aws_rds.StorageType.GP2,
        publiclyAccessible: true,
        // No custom parameter group needed
        iamAuthentication: false
      },
    );
    
    // Note: pgvector extension is available in PostgreSQL 16
    // After deployment, connect to the database and run:
    // CREATE EXTENSION IF NOT EXISTS pgvector;
    
    // Add an output to remind about pgvector setup
    new CfnOutput(this, "PgVectorInstructions", {
      value: "To enable pgvector, connect to the database and run: CREATE EXTENSION IF NOT EXISTS pgvector;",
      description: "Instructions for enabling pgvector extension",
    });

    // Step 6: Create Secrets Manager Secret for JWT
    const jwtKmsKey = new aws_kms.Key(this, `${prefix}-JwtKmsKey`, {
      alias: `${prefix}-JwtKmsKey`,
      removalPolicy: cdk.RemovalPolicy.DESTROY,
    });
    const jwtSecret = new aws_secretsmanager.Secret(this, `${prefix}-JwtSecret`, {
      secretName: `${prefix}-JwtSecret`,
      description: "Secret for JWT token encryption",
      encryptionKey: jwtKmsKey,
      generateSecretString: {
        secretStringTemplate: JSON.stringify({ key: "jwt-secret-key" }),
        generateStringKey: "JWT_SECRET",
        excludeCharacters: "\"@/\\ '",
      },
    });

    // Retrieve the secret value for the password
    const dbPassword = dbCredentialsSecret
      .secretValueFromJson("password")
      .unsafeUnwrap(); // For demonstration; in practice, handle secrets securely

    // Retrieve the secret value for the username
    const dbUsername = dbCredentialsSecret
      .secretValueFromJson("username")
      .unsafeUnwrap(); // For demonstration; in practice, handle secrets securely

    // Create an ECS cluster
    const cluster = new aws_ecs.Cluster(this, `${prefix}-cluster`, {
      vpc: vpc,
    });

    const logName = `${prefix}-logs`;

    // Create an IAM Role for Fargate Task
    const fargateTaskRole = new aws_iam.Role(this, `${prefix}-iam-role`, {
      assumedBy: new aws_iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
    });

    const logGroup = new aws_logs.LogGroup(this, logName, {
      logGroupName: logName,
      removalPolicy: cdk.RemovalPolicy.DESTROY, // Adjust the removal policy as needed
    });

    // Define the ECS Task Definition
    const taskDefinition = new aws_ecs.FargateTaskDefinition(
      this,
      `${prefix}-ecs-fargate-definition`,
      {
        memoryLimitMiB: 8192, // Increased from 4096 to 8192 (8GB)
        cpu: 2048,           // Increased from 1024 to 2048 (2 vCPU)
        taskRole: fargateTaskRole,
        executionRole: fargateTaskRole,
        runtimePlatform: {
          operatingSystemFamily: aws_ecs.OperatingSystemFamily.LINUX,
          cpuArchitecture: aws_ecs.CpuArchitecture.ARM64,
        },
      },
    );

    // Function to parse the image URI
    function getRepositoryArn(imageUri: string): string {
      const [registryPart, repoWithTag] = imageUri.split(".amazonaws.com/");
      const [accountId, , , region] = registryPart.split(".");
      const [repositoryName] = repoWithTag.split(":");

      return `arn:aws:ecr:${region}:${accountId}:repository/${repositoryName}`;
    }

    const repository = aws_ecr.Repository.fromRepositoryArn(
      this,
      `${prefix}-container-repo`,
      getRepositoryArn(ecrRepo),
    );

    // Determine the domain name based on environment
    const apiDomainName = environment === 'production' ? 'api.autoturn.ai' : `${environment}-api.autoturn.ai`;
    // Create Route53 record for the frontend
    const frontendDomainName = environment === 'production' ? 'app.autoturn.ai' : `${environment}-app.autoturn.ai`;


    // Add container to the task definition
    
    const container = taskDefinition.addContainer(`${prefix}-container`, {
      image: aws_ecs.ContainerImage.fromEcrRepository(repository, version),
      logging: aws_ecs.LogDrivers.awsLogs({
        streamPrefix: `${prefix}-logs`,
        logGroup: logGroup,
      }),
      environment: {
        PGSSLMODE: 'require', // Force PostgreSQL SSL in client library
        POSTGRES_DB: POSTGRES_DB ?? 'wholesale',
        POSTGRES_PORT: '5432',
        NODE_ENV: 'production',
        POSTGRES_USER: dbUsername,
        POSTGRES_PASSWORD: dbPassword,
        POSTGRES_HOST: dbInstance.dbInstanceEndpointAddress,
        DATABASE_URL: `postgres://${dbUsername}:${dbPassword}@${dbInstance.dbInstanceEndpointAddress}:5432/${POSTGRES_DB}`,
        OPENAI_API_KEY: OPENAI_API_KEY ?? '',
        ECR_REPO: ecrRepo,
        SESSION_SECRET: SESSION_SECRET ?? '',
        SEND_GRID_API_KEY: SENDGRID_API_KEY ?? '',
        SEND_GRID_FROM_EMAIL: SEND_GRID_FROM_EMAIL ?? '',
        ENV: ENV ?? '',
        DOMAIN_NAME: DOMAIN_NAME ?? '',
        APP_NAME: APP_NAME ?? '',
        API_URL: 'https://'+apiDomainName,
        FRONT_END_URL: 'https://'+frontendDomainName,
        EMAIL: EMAIL ?? '',
        POSTMARK_API_KEY: POSTMARK_API_KEY ?? '',
        JWT_SECRET: jwtSecret.secretValueFromJson("JWT_SECRET").unsafeUnwrap() ?? ''
      }
    });

    /**
     * # API Configuration
API_PORT=3005
API_INTERNAL_PORT=3005
API_URL=http://localhost:3005

# Frontend Configuration
WEB_PORT=3006
WEB_INTERNAL_PORT=3006

# CORS Configuration
CORS_ORIGIN=http://localhost:3006
     */

    container.addPortMappings({
      containerPort: 3005,
      hostPort: 3005,
      protocol: aws_ecs.Protocol.TCP,
    });

    // Create the ECS Fargate service
    const ecsService = new aws_ecs.FargateService(
      this,
      `${prefix}-ecs-service`,
      {
        cluster,
        taskDefinition,
        desiredCount: 3,  // Increased from 2 to 3 initial tasks
        securityGroups: [ecsSecurityGroup],
        assignPublicIp: true,
        platformVersion: aws_ecs.FargatePlatformVersion.VERSION1_4,
      },
    );

    // Auto Scaling Configuration
    const scaling = ecsService.autoScaleTaskCount({ 
      maxCapacity: 12,     // Increased from 10 to 12 max tasks
      minCapacity: 2       // Added minimum capacity
    });
    
    // CPU-based scaling
    scaling.scaleOnCpuUtilization("CpuScaling", {
      targetUtilizationPercent: 60,  // Lowered from 70% to 60% to scale earlier
      scaleInCooldown: cdk.Duration.minutes(5),
      scaleOutCooldown: cdk.Duration.minutes(3),  // Reduced scale out cooldown for faster response
    });

    // Memory-based scaling
    scaling.scaleOnMemoryUtilization("MemoryScaling", {
      targetUtilizationPercent: 65,  // Scale when memory usage hits 65%
      scaleInCooldown: cdk.Duration.minutes(5),
      scaleOutCooldown: cdk.Duration.minutes(3),
    });

    const lb = new aws_elasticloadbalancingv2.ApplicationLoadBalancer(
      this,
      `${prefix}-lb`,
      {
        vpc,
        internetFacing: true,
        vpcSubnets: {
          subnetType: aws_ec2.SubnetType.PUBLIC,
          onePerAz: true,
        },
      },
    );

    // Import existing hosted zone by domain name
    const hostedZone = aws_route53.HostedZone.fromLookup(this, `${prefix}-hosted-zone`, {
      domainName: 'autoturn.ai'
    });

    // Find and import existing certificate for the domain
    const certificate = aws_certificatemanager.Certificate.fromCertificateArn(
      this,
      `${prefix}-certificate`,
      'arn:aws:acm:us-east-1:626635413051:certificate/50869182-d5c2-41b6-9b58-8cd58b1fe498'
    );

    // HTTP Listener (redirects to HTTPS)
    const httpListener = lb.addListener(`${prefix}-http-listener`, {
      port: 80,
      protocol: aws_elasticloadbalancingv2.ApplicationProtocol.HTTP,
      defaultAction: aws_elasticloadbalancingv2.ListenerAction.redirect({
        port: "443",
        protocol: aws_elasticloadbalancingv2.ApplicationProtocol.HTTPS,
        permanent: true,
      }),
    });

    // HTTPS Listener
    const targetGroup = new aws_elasticloadbalancingv2.ApplicationTargetGroup(this, `${prefix}-target-group`, {
      vpc,
      port: 3005,
      protocol: aws_elasticloadbalancingv2.ApplicationProtocol.HTTP,
      targets: [ecsService],
      healthCheck: {
        path: "/health",
        interval: cdk.Duration.minutes(5),
        timeout: cdk.Duration.seconds(10),
        healthyThresholdCount: 2,
        unhealthyThresholdCount: 3,
        healthyHttpCodes: "200",
        port: "3005"
      },
    });

    const httpsListener = lb.addListener(`${prefix}-https-listener`, {
      port: 443,
      protocol: aws_elasticloadbalancingv2.ApplicationProtocol.HTTPS,
      certificates: [certificate],
      defaultAction: aws_elasticloadbalancingv2.ListenerAction.forward([targetGroup])
    });


    // Create A record in Route53
    new aws_route53.ARecord(this, `${prefix}-alias-record`, {
      zone: hostedZone,
      target: aws_route53.RecordTarget.fromAlias(
        new aws_route53_targets.LoadBalancerTarget(lb)
      ),
      recordName: apiDomainName
    });


    // Grant permissions to the task role
    logGroup.grantWrite(taskDefinition.taskRole);

    // Output the domain name
    new CfnOutput(this, "DomainName", {
      value: `https://${apiDomainName}`,
      description: "The domain name of the application",
    });

    // Output the load balancer DNS name
    new CfnOutput(this, "LoadBalancerDNS", {
      value: lb.loadBalancerDnsName,
      description: "The DNS name of the load balancer",
    });

    // Create S3 bucket for hosting the frontend
    const websiteBucket = new aws_s3.Bucket(this, `${prefix}-website-bucket`, {
      bucketName: `${prefix}-frontend`,
      removalPolicy: cdk.RemovalPolicy.DESTROY,
      autoDeleteObjects: true,
      blockPublicAccess: aws_s3.BlockPublicAccess.BLOCK_ALL,
    });

    // Create CloudFront Origin Access Identity (OAI)
    const oai = new aws_cloudfront.OriginAccessIdentity(this, `${prefix}-oai`, {
      comment: `OAI for ${prefix} website bucket`,
    });
    // Grant CloudFront OAI read access to S3 bucket
    websiteBucket.grantRead(oai);

    // Create CloudFront distribution
    const distribution = new aws_cloudfront.Distribution(this, `${prefix}-distribution`, {
      defaultRootObject: 'index.html',
      defaultBehavior: {
        origin: new aws_cloudfront_origins.S3Origin(websiteBucket, {
          originAccessIdentity: oai,
        }),
        compress: true,
        allowedMethods: aws_cloudfront.AllowedMethods.ALLOW_GET_HEAD_OPTIONS,
        viewerProtocolPolicy: aws_cloudfront.ViewerProtocolPolicy.REDIRECT_TO_HTTPS,
        cachePolicy: aws_cloudfront.CachePolicy.CACHING_OPTIMIZED,
      },
      certificate: certificate,
      domainNames: [frontendDomainName],
      errorResponses: [
        { httpStatus: 404, responseHttpStatus: 200, responsePagePath: '/index.html' },
        { httpStatus: 403, responseHttpStatus: 200, responsePagePath: '/index.html' },
      ],
    });

    // Deploy the frontend to S3
    new aws_s3_deployment.BucketDeployment(this, `${prefix}-website-deployment`, {
      sources: [aws_s3_deployment.Source.asset('../web-front-end/dist')],
      destinationBucket: websiteBucket,
    });
    
    new aws_route53.ARecord(this, `${prefix}-frontend-alias`, {
      zone: hostedZone,
      target: aws_route53.RecordTarget.fromAlias(
        new aws_route53_targets.CloudFrontTarget(distribution)
      ),
      recordName: frontendDomainName,
    });

    // Output the frontend URL
    new CfnOutput(this, "FrontendURL", {
      value: `https://${frontendDomainName}`,
      description: "The URL of the frontend application",
    });

    // Output the CloudFront distribution ID
    new CfnOutput(this, "CloudFrontDistributionId", {
      value: distribution.distributionId,
      description: "The ID of the CloudFront distribution",
    });
  }
}
```
